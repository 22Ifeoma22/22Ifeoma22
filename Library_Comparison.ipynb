{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/22Ifeoma22/22Ifeoma22/blob/main/Library_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1my-S0PX9CB"
      },
      "source": [
        "# Machine Learning with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnVT_1SAX9CD"
      },
      "source": [
        "## Comparing Machine Learning Libraries\n",
        "\n",
        "For this overview example, we will create a classification model using:\n",
        "\n",
        "1. scikit-learn\n",
        "2. Keras\n",
        "3. PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71j2W6I9X9CD"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7VwZaMgX9CE"
      },
      "source": [
        "## scikit-learn style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbLXw-wKX9CE"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LTTDqvqX9CE",
        "outputId": "a3c75c72-606d-4ddc-bda6-bd1c83e62aa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data (rows, features): (569, 30)\n"
          ]
        }
      ],
      "source": [
        "cancer = load_breast_cancer()\n",
        "X_scaled = StandardScaler().fit_transform(cancer.data)\n",
        "print(\"Original data (rows, features):\", X_scaled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PL_JlPt7X9CF",
        "outputId": "86b273fd-7ef2-4c79-a5dd-86be58c23fda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All polynomial features (order 2): (569, 496)\n",
            "CPU times: user 5.84 ms, sys: 3.03 ms, total: 8.87 ms\n",
            "Wall time: 20.6 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Generating to polynomial features is not that time consuming\n",
        "poly = PolynomialFeatures(2)\n",
        "X_poly = poly.fit_transform(X_scaled)\n",
        "print(\"All polynomial features (order 2):\", X_poly.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLt51y9pX9CF",
        "outputId": "f4ae3d4a-e0cf-41ef-fe0c-44926db30c29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best polynomial features (569, 475)\n",
            "CPU times: user 2.38 s, sys: 273 ms, total: 2.66 s\n",
            "Wall time: 1min 46s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# A fairly generic random forest\n",
        "rfc = RandomForestClassifier(max_depth=7, n_estimators=10, random_state=1)\n",
        "\n",
        "# Do some work to pick the optimal number of features\n",
        "# \"Recursive feature elimination using cross-validation\"\n",
        "rfecv = RFECV(estimator=rfc, cv=5, n_jobs=-1)\n",
        "X_poly_top = rfecv.fit_transform(X_poly, cancer.target)\n",
        "\n",
        "# The \"top\" features selected for the model\n",
        "print(\"Best polynomial features\", X_poly_top.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIgvupMAX9CF",
        "outputId": "d57b2da7-fba7-4447-f2d0-51fd616ab170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.951048951048951\n",
            "CPU times: user 57.3 ms, sys: 2.07 ms, total: 59.4 ms\n",
            "Wall time: 59.8 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Do a train/test split on the \"poly_top\" features\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_poly_top, cancer.target, random_state=42)\n",
        "\n",
        "# Train the selected RFC model\n",
        "rfc = RandomForestClassifier(max_depth=7, n_estimators=10, random_state=1)\n",
        "print(\"Test accuracy:\", rfc.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuO1UKodX9CF"
      },
      "source": [
        "## Neural Networks\n",
        "\n",
        "There are several things to notice in our NN setup.  We do *not* generate polynomial features.  Instead, we allow the network itself to derive them on a first layer we arrange to have the same number of neurons as there were polynomial features in our Random Forest approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3ORekh0X9CF",
        "outputId": "799a5978-5c36-44c9-e73d-0bc131693473",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "batch_size = 32\n",
        "in_dim = cancer.data.shape[1]\n",
        "hidden1 = X_poly_top.shape[1]   # The size of layer that deduces poly features\n",
        "hidden2 = 20                    # The size of the \"inference layer\"\n",
        "out_dim = 1                     # Output a single value\n",
        "\n",
        "batches_in_data = X_train.shape[0]/batch_size\n",
        "epochs = int(5000/batches_in_data)\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# Split the original data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "                           cancer.data, cancer.target, random_state=42)\n",
        "cancer.data.shape   # The shape of the data being split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82wA4b_6X9CG"
      },
      "source": [
        "## Keras style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V7eIyhHX9CG"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nflM_1TX9CG"
      },
      "outputs": [],
      "source": [
        "model_k = keras.models.Sequential([\n",
        "    # This layer allows \"polynomial features\"\n",
        "    keras.layers.Dense(hidden1, activation='relu', input_shape=(in_dim,)),\n",
        "    # This layer is the essential \"inference\"\n",
        "    keras.layers.Dense(hidden2),\n",
        "    # Often Leaky ReLU eliminates the \"dead neuron\" danger\n",
        "    keras.layers.LeakyReLU(),\n",
        "    # A Dropout layer sometimes reduces co-adaptation of neurons\n",
        "    keras.layers.Dropout(rate=0.25),\n",
        "    # A sigmoid activation is used for a binary decision\n",
        "    keras.layers.Dense(out_dim, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFs-lGyNX9CG",
        "outputId": "6d8c26fc-6357-4049-c927-6b54b5f4fd00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 475)               14725     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20)                9520      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 20)                0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 20)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24266 (94.79 KB)\n",
            "Trainable params: 24266 (94.79 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_k.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUuwWwMJX9CG",
        "outputId": "52af3199-1287-4a3d-8fe4-7a9e67849b04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 3ms/step - loss: 0.3776 - accuracy: 0.6224\n",
            "Test loss: 0.3776223659515381\n",
            "Test accuracy: 0.6223776340484619\n",
            "CPU times: user 26.9 s, sys: 1.21 s, total: 28.1 s\n",
            "Wall time: 41.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# The default optimization is Root Mean Square Propogation\n",
        "model_k.compile(loss='mean_squared_error',\n",
        "                optimizer=keras.optimizers.RMSprop(lr=learning_rate),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "history = model_k.fit(X_train, y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs,\n",
        "                      verbose=False,\n",
        "                      validation_data=(X_test, y_test))\n",
        "\n",
        "score = model_k.evaluate(X_test, y_test, verbose=True)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpHqKdpIX9CH",
        "outputId": "2c75331a-742c-4d4c-aba5-80cd99b3b680",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 4ms/step - loss: 0.3776 - accuracy: 0.6224\n",
            "Test loss: 0.3776223659515381\n",
            "Test accuracy: 0.6223776340484619\n",
            "CPU times: user 27.6 s, sys: 1.11 s, total: 28.7 s\n",
            "Wall time: 41.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Sometimes we do better using Adaptive Moment Optimization\n",
        "model_k.compile(loss='mean_squared_error',\n",
        "                optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "history = model_k.fit(X_train, y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs,\n",
        "                      verbose=False,\n",
        "                      validation_data=(X_test, y_test))\n",
        "score = model_k.evaluate(X_test, y_test, verbose=True)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg15rs0WX9CH"
      },
      "source": [
        "## PyTorch style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuQkk6FoX9CH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wzRBRFqX9CH"
      },
      "outputs": [],
      "source": [
        "# Create a sequential NN\n",
        "model_t = torch.nn.Sequential(\n",
        "    # This layer allows \"polynomial features\"\n",
        "    torch.nn.Linear(in_dim, hidden1),\n",
        "    # The activation is treated as a separate layer\n",
        "    torch.nn.ReLU(),\n",
        "    # This layer is the essential \"inference\"\n",
        "    torch.nn.Linear(hidden1, hidden2),\n",
        "    # Often Leaky ReLU eliminates the \"dead neuron\" danger\n",
        "    torch.nn.LeakyReLU(),\n",
        "    # A Dropout layer sometimes reduces co-adaptation of neurons\n",
        "    torch.nn.Dropout(p=0.25),\n",
        "    # A sigmoid activation is used for a binary decision\n",
        "    torch.nn.Linear(hidden2, out_dim),\n",
        "    torch.nn.Sigmoid()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "AJt2yXS0X9CH",
        "outputId": "b11b9ddf-3183-423f-ba83-8739e644adb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1               [-1, 1, 475]          14,725\n",
            "              ReLU-2               [-1, 1, 475]               0\n",
            "            Linear-3                [-1, 1, 20]           9,520\n",
            "         LeakyReLU-4                [-1, 1, 20]               0\n",
            "           Dropout-5                [-1, 1, 20]               0\n",
            "            Linear-6                 [-1, 1, 1]              21\n",
            "           Sigmoid-7                 [-1, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 24,266\n",
            "Trainable params: 24,266\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 0.09\n",
            "Estimated Total Size (MB): 0.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torch import device, cuda\n",
        "from torchsummary import summary\n",
        "\n",
        "# torchsummary has a glitch. If running on a CUDA-enabled build\n",
        "# it only wants to print a CUDA model\n",
        "if cuda.is_available():\n",
        "    model_t = model_t.to(device('cuda'))\n",
        "\n",
        "summary(model_t, input_size=(1,in_dim))\n",
        "\n",
        "# model_t = model_t.to(device('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgmw5WU4X9CH"
      },
      "outputs": [],
      "source": [
        "show_every = 250\n",
        "\n",
        "def do_training():\n",
        "    for t in range(5000):\n",
        "        # Forward pass: compute predicted y by passing x to the model.\n",
        "        y_pred = model_t(X)\n",
        "\n",
        "        # Compute and print loss.\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        if not t % show_every:\n",
        "            y_test_pred = model_t(Variable(X_test_T))\n",
        "            prediction = [int(x > 0.5)\n",
        "                          for x in y_test_pred.data.cpu().numpy()]\n",
        "            test_accuracy = (prediction == y_test).sum() / len(y_test)\n",
        "            train_pred = [int(x > 0.5)\n",
        "                          for x in y_pred.data.cpu().numpy()]\n",
        "            train_accuracy = (train_pred == y_train).sum() / len(y_train)\n",
        "            print(\"Batch: %04d | Training Loss: %6.2f | Train accuracy: %.4f | Test accuracy: %.4f\" % (\n",
        "                          t, loss.item(), train_accuracy, test_accuracy))\n",
        "\n",
        "        # Before the backward pass, use the optimizer object to zero all of the\n",
        "        # gradients for the variables it will update (which are the learnable\n",
        "        # weights of the model). This is because by default, gradients are\n",
        "        # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "        # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass: compute gradient of the loss with respect to model\n",
        "        # parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Calling the step function on an Optimizer makes an update to its\n",
        "        # parameters\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E21Qi_y0X9CH",
        "outputId": "afce2645-37c8-460a-c5dd-b2eebdd12fb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: 0000 | Training Loss: 222.72 | Train accuracy: 0.4390 | Test accuracy: 0.4615\n",
            "Batch: 0250 | Training Loss:  27.90 | Train accuracy: 0.9131 | Test accuracy: 0.9510\n",
            "Batch: 0500 | Training Loss:  25.96 | Train accuracy: 0.9178 | Test accuracy: 0.9441\n",
            "Batch: 0750 | Training Loss:  22.75 | Train accuracy: 0.9272 | Test accuracy: 0.9510\n",
            "Batch: 1000 | Training Loss:  22.09 | Train accuracy: 0.9249 | Test accuracy: 0.9441\n",
            "Batch: 1250 | Training Loss:  24.19 | Train accuracy: 0.9249 | Test accuracy: 0.9510\n",
            "Batch: 1500 | Training Loss:  20.58 | Train accuracy: 0.9366 | Test accuracy: 0.9371\n",
            "Batch: 1750 | Training Loss:  22.38 | Train accuracy: 0.9343 | Test accuracy: 0.9441\n",
            "Batch: 2000 | Training Loss:  19.15 | Train accuracy: 0.9390 | Test accuracy: 0.9580\n",
            "Batch: 2250 | Training Loss:  20.50 | Train accuracy: 0.9413 | Test accuracy: 0.9301\n",
            "Batch: 2500 | Training Loss:  16.77 | Train accuracy: 0.9484 | Test accuracy: 0.9510\n",
            "Batch: 2750 | Training Loss:  18.09 | Train accuracy: 0.9437 | Test accuracy: 0.9371\n",
            "Batch: 3000 | Training Loss:  17.30 | Train accuracy: 0.9484 | Test accuracy: 0.9371\n",
            "Batch: 3250 | Training Loss:  17.91 | Train accuracy: 0.9507 | Test accuracy: 0.9441\n",
            "Batch: 3500 | Training Loss:  15.55 | Train accuracy: 0.9413 | Test accuracy: 0.9441\n",
            "Batch: 3750 | Training Loss:  15.07 | Train accuracy: 0.9507 | Test accuracy: 0.9650\n",
            "Batch: 4000 | Training Loss:  19.29 | Train accuracy: 0.9272 | Test accuracy: 0.9510\n",
            "Batch: 4250 | Training Loss:  17.64 | Train accuracy: 0.9437 | Test accuracy: 0.9301\n",
            "Batch: 4500 | Training Loss:  14.68 | Train accuracy: 0.9484 | Test accuracy: 0.9441\n",
            "Batch: 4750 | Training Loss:  16.71 | Train accuracy: 0.9484 | Test accuracy: 0.9301\n",
            "CPU times: user 22.7 s, sys: 423 ms, total: 23.1 s\n",
            "Wall time: 26.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "## Now run model\n",
        "X = torch.from_numpy(X_train).float()\n",
        "y = torch.from_numpy(y_train[:, np.newaxis]).float()\n",
        "X_test_T = torch.from_numpy(X_test).float()\n",
        "y_test_T = torch.from_numpy(y_test[:, np.newaxis]).float()\n",
        "\n",
        "if cuda.is_available():\n",
        "    X = X.to(device('cuda'))\n",
        "    y = y.to(device('cuda'))\n",
        "    X_test_T = X_test_T.to(device('cuda'))\n",
        "    y_test_T = y_test_T.to(device('cuda'))\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.RMSprop(model_t.parameters(), lr=learning_rate)\n",
        "do_training()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qj-23nCX9CI",
        "outputId": "4d009a27-a4ba-4b61-f13d-b14336a45da9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: 0000 | Training Loss:  13.66 | Train accuracy: 0.9577 | Test accuracy: 0.9441\n",
            "Batch: 0250 | Training Loss:  10.05 | Train accuracy: 0.9742 | Test accuracy: 0.9510\n",
            "Batch: 0500 | Training Loss:   9.78 | Train accuracy: 0.9718 | Test accuracy: 0.9371\n",
            "Batch: 0750 | Training Loss:   6.60 | Train accuracy: 0.9812 | Test accuracy: 0.9510\n",
            "Batch: 1000 | Training Loss:   6.19 | Train accuracy: 0.9906 | Test accuracy: 0.9510\n",
            "Batch: 1250 | Training Loss:   4.69 | Train accuracy: 0.9906 | Test accuracy: 0.9510\n",
            "Batch: 1500 | Training Loss:   5.05 | Train accuracy: 0.9930 | Test accuracy: 0.9580\n",
            "Batch: 1750 | Training Loss:   3.19 | Train accuracy: 0.9906 | Test accuracy: 0.9510\n",
            "Batch: 2000 | Training Loss:   3.30 | Train accuracy: 0.9930 | Test accuracy: 0.9510\n",
            "Batch: 2250 | Training Loss:   3.45 | Train accuracy: 0.9930 | Test accuracy: 0.9650\n",
            "Batch: 2500 | Training Loss:   2.23 | Train accuracy: 0.9977 | Test accuracy: 0.9510\n",
            "Batch: 2750 | Training Loss:   2.99 | Train accuracy: 0.9930 | Test accuracy: 0.9441\n",
            "Batch: 3000 | Training Loss:   1.72 | Train accuracy: 1.0000 | Test accuracy: 0.9720\n",
            "Batch: 3250 | Training Loss:   1.66 | Train accuracy: 0.9977 | Test accuracy: 0.9510\n",
            "Batch: 3500 | Training Loss:   1.79 | Train accuracy: 0.9953 | Test accuracy: 0.9580\n",
            "Batch: 3750 | Training Loss:   2.14 | Train accuracy: 0.9930 | Test accuracy: 0.9580\n",
            "Batch: 4000 | Training Loss:   0.91 | Train accuracy: 1.0000 | Test accuracy: 0.9580\n",
            "Batch: 4250 | Training Loss:   0.85 | Train accuracy: 1.0000 | Test accuracy: 0.9580\n",
            "Batch: 4500 | Training Loss:   0.94 | Train accuracy: 1.0000 | Test accuracy: 0.9580\n",
            "Batch: 4750 | Training Loss:   0.78 | Train accuracy: 1.0000 | Test accuracy: 0.9650\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model_t.parameters(), lr=learning_rate)\n",
        "do_training()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1QJrn5xX9CI"
      },
      "source": [
        "### Make a few predictions with trained model\n",
        "\n",
        "Run the below code several times.  Because it uses a Dropout layer, the activated neurons—and hence the exact predictions—will vary on each call.  Ideally the results will be consistent in identifying the binary class, but they will not be precisely identical in floating point value output in range `[0,1]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFtzrKAyX9CI",
        "outputId": "e82035b1-8e49-4546-a4c6-abe507db6fe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation 0; probability benign: 99.894%\n",
            "Observation 1; probability benign: 0.000%\n",
            "Observation 2; probability benign: 0.000%\n",
            "Observation 3; probability benign: 100.000%\n",
            "Observation 4; probability benign: 100.000%\n",
            "Observation 5; probability benign: 0.000%\n",
            "Observation 6; probability benign: 0.000%\n",
            "Observation 7; probability benign: 86.930%\n",
            "Observation 8; probability benign: 99.324%\n",
            "Observation 9; probability benign: 99.994%\n"
          ]
        }
      ],
      "source": [
        "predictions = model_t(X_test_T[:10])\n",
        "for row, prediction in enumerate(predictions):\n",
        "    print(\"Observation %d; probability benign: %0.3f%%\" % (row, prediction*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuEXf4i3X9CI"
      },
      "source": [
        "## Classifying an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J3Bw7m2RX9CI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf5bd06-8af8-486d-f803-8773214c7cbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 131MB/s] \n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "from torchvision.transforms import Resize, ToTensor, Compose\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "\n",
        "inception = models.inception_v3(pretrained=True).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0MDBq2uZX9CI"
      },
      "outputs": [],
      "source": [
        "# Load the imagenet labels for 1000 pre-trained image classes\n",
        "class_defs = json.load(open(\"/content/sample_data/imagenet_class_index.json\"))\n",
        "labels = {int(k):name for k, (code, name) in class_defs.items()}\n",
        "\n",
        "# Small utility to load, resize and tensorize images\n",
        "def load_images(fnames):\n",
        "    for fname in fnames:\n",
        "        image = Image.open(fname)\n",
        "        image_t = Compose([Resize(299), ToTensor()])(image).float()\n",
        "        image_t = torch.tensor(image_t, requires_grad=True)\n",
        "        yield image, image_t.unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8aeAYoKqX9CI",
        "outputId": "1e61972c-8e98-4693-e143-8e66abaea9c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'img/cannot-brain.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a7d3f4240e1f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m for image, image_tensor in load_images([\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0;34m'img/cannot-brain.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0;34m'img/rainbow-butterfly-unicorn-kitten.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;34m'img/Crisopid_July_2013-9.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             'img/dqm-bokeh-palms.jpg']):\n",
            "\u001b[0;32m<ipython-input-3-dd376a420970>\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(fnames)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mimage_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mimage_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'img/cannot-brain.jpg'"
          ]
        }
      ],
      "source": [
        "for image, image_tensor in load_images([\n",
        "            'img/cannot-brain.jpg',\n",
        "            'img/rainbow-butterfly-unicorn-kitten.jpg',\n",
        "            'img/Crisopid_July_2013-9.jpg',\n",
        "            'img/dqm-bokeh-palms.jpg']):\n",
        "    outputs = inception(image_tensor)\n",
        "    prediction = np.argmax(outputs.detach().numpy())\n",
        "    display(image)\n",
        "    print(labels[prediction])\n",
        "    print('—'*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q30_EzdI0D5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import os\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "def load_images(fnames):\n",
        "    for fname in fnames:\n",
        "        # Construct the absolute path to the image\n",
        "        image_path = os.path.join('img/cannot-brain.jpg', fname)\n",
        "\n",
        "        # Check if the file exists\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Error: File not found - {image_path}\")\n",
        "            continue  # Skip to the next image\n",
        "\n",
        "        image = Image.open(image_path)\n",
        "        image_t = Compose([Resize(299), ToTensor()])(image).float()\n",
        "        image_t = torch.tensor(image_t, requires_grad=True)\n",
        "        yield image, image_t.unsqueeze(0)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "z9b96R0K0CXW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MTFd3uUyRzZ",
        "outputId": "68aa1710-1b90-4da2-fdfd-aa37dad035ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVEicYWNX9CJ"
      },
      "source": [
        "## Next Lesson\n",
        "\n",
        "**Diving Deeper**: We have seen a few brief examples of PyTorch in use, and illustrated a little bit about how its APIs differ from those of other libraries.  Next we will look at some of the essential concept in the design of PyTorch.\n",
        "\n",
        "<a href=\"IntroPyTorch.ipynb\"><img src=\"img/open-notebook.png\" align=\"left\"/></a>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}